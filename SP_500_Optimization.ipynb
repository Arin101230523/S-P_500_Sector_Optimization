{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installs necessary libraries\n",
    "!pip install openpyxl\n",
    "!pip install pandas\n",
    "!pip install yfinance\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "!pip install PyPortfolioOpt\n",
    "!pip install cvxpy\n",
    "\n",
    "sector_allocation = {\n",
    "    ''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sci_opt\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set some display options for Pandas.\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "## Fetching S&P 500 Companies:\n",
    "sp500_companies = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "sp500_companies['Symbol'] = sp500_companies['Symbol'].replace({'BRK.B': 'BRK-B', 'BF.B': 'BF-B'})\n",
    "sp500_companies\n",
    "# Dropping NaN values\n",
    "sp500_companies.dropna(inplace=True)\n",
    "## Getting Unique Sectors and Dates:\n",
    "unique_sectors = sp500_companies['GICS Sector'].unique()\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.today() - timedelta(days=90)).strftime('%Y-%m-%d')\n",
    "unique_sectors_df = pd.DataFrame(unique_sectors)\n",
    "print(unique_sectors_df)\n",
    "number_of_sectors = len(unique_sectors)\n",
    "len(unique_sectors)\n",
    "\n",
    "## Fetching Historical Prices and Calculating Averages:\n",
    "## The code fetches historical closing prices for each company in each sector and calculates the average closing price for the sector.\n",
    "# Loop through each sector and download closing prices\n",
    "all_avg_prices = []\n",
    "all_dates = []\n",
    "all_sectors = []\n",
    "for sector in unique_sectors:\n",
    "    companies = sp500_companies[sp500_companies['GICS Sector'] == sector]['Symbol'].tolist()\n",
    "    prices = yf.download(companies, start=start_date, end=end_date)['Adj Close']\n",
    "    avg_prices = prices.mean(axis=1)\n",
    "    # Append the data\n",
    "    all_avg_prices.extend(avg_prices.tolist())\n",
    "    all_dates.extend([date.date() for date in prices.index])\n",
    "    all_sectors.extend([sector] * len(avg_prices))\n",
    "    \n",
    "# Create a DataFrame\n",
    "sp500_sectors_avg_prices = pd.DataFrame({\n",
    "    'Date': all_dates,\n",
    "    'Avg_Close': all_avg_prices,\n",
    "    'Sector': all_sectors\n",
    "})\n",
    "\n",
    "\n",
    "#sp500_sectors_avg_prices = sp500_sectors_avg_prices.pivot(index='Date', columns='Sector', values='Avg_Close')\n",
    "# Reverse the order of the DataFrame by dates\n",
    "sp500_sectors_avg_prices = sp500_sectors_avg_prices[::-1]\n",
    "\n",
    "print(sp500_sectors_avg_prices.head(100))\n",
    "sp500_sectors_avg_prices = sp500_sectors_avg_prices.pivot(\n",
    "    index='Date',\n",
    "    columns='Sector',\n",
    "    values='Avg_Close'\n",
    ")\n",
    "print(sp500_sectors_avg_prices.head())\n",
    "sp500_sectors_avg_prices.dropna(inplace=True)\n",
    "print(sp500_sectors_avg_prices.isnull().sum())\n",
    "sp500_sectors_avg_prices.to_excel('sp500_sectors_avg_prices.xlsx', index=True)\n",
    "\n",
    "# Calculate the Log of returns.\n",
    "log_return = np.log(1 +sp500_sectors_avg_prices.pct_change().iloc[::-1])\n",
    "\n",
    "# Drop rows with negative values\n",
    "log_return = log_return[(log_return > 0).all(axis=1)]\n",
    "\n",
    "\n",
    "# Generate Random Weights.\n",
    "random_weights = np.array(np.random.random(number_of_sectors))\n",
    "\n",
    "# Generate the Rebalance Weights, these should equal 1.\n",
    "rebalance_weights = random_weights / np.sum(random_weights)\n",
    "\n",
    "print('Log Returns:')\n",
    "print(log_return.head())\n",
    "print((sp500_sectors_avg_prices == 0).sum())\n",
    "log_return.to_excel('log_return.xlsx', index=True)\n",
    "# Calculate the percentage of negative log returns\n",
    "percentage_negative_returns = (log_return < 0).mean().mean() * 100\n",
    "\n",
    "print(f\"Percentage of negative log returns: {percentage_negative_returns:.2f}%\")\n",
    "\n",
    "\n",
    "# Calculate the Expected Returns, annualize it by multiplying it by `252`.\n",
    "risk_free_rate = .01\n",
    "exp_ret = np.sum(((log_return.mean()-risk_free_rate) * rebalance_weights) * 252)\n",
    "\n",
    "# Calculate the Expected Volatility, annualize it by multiplying it by `252`.\n",
    "exp_vol = np.sqrt(\n",
    "np.dot(\n",
    "    rebalance_weights.T,\n",
    "    np.dot(\n",
    "        log_return.cov() * 252,\n",
    "        rebalance_weights\n",
    "    )\n",
    ")\n",
    ")\n",
    "\n",
    "# Calculate the Sharpe Ratio.\n",
    "sharpe_ratio = exp_ret / exp_vol\n",
    "\n",
    "# Put the weights into a data frame to see them better.\n",
    "weights_df = pd.DataFrame(data={\n",
    "'random_weights': random_weights,\n",
    "'rebalance_weights': rebalance_weights\n",
    "})\n",
    "print('')\n",
    "print('='*80)\n",
    "print('PORTFOLIO WEIGHTS:')\n",
    "print('-'*80)\n",
    "print(weights_df)\n",
    "print('-'*80)\n",
    "\n",
    "# Do the same with the other metrics.\n",
    "metrics_df = pd.DataFrame(data={\n",
    "    'Expected Portfolio Returns': exp_ret,\n",
    "    'Expected Portfolio Volatility': exp_vol,\n",
    "    'Portfolio Sharpe Ratio': sharpe_ratio\n",
    "}, index=[0])\n",
    "\n",
    "print('')\n",
    "print('='*80)\n",
    "print('PORTFOLIO METRICS:')\n",
    "print('-'*80)\n",
    "print(metrics_df)\n",
    "print('-'*80)\n",
    "\n",
    "# Initialize the components, to run a Monte Carlo Simulation.\n",
    "\n",
    "# We will run 5000 iterations.\n",
    "num_of_portfolios = 20000\n",
    "\n",
    "# Prep an array to store the weights as they are generated, 5000 iterations for each of our 4 symbols.\n",
    "all_weights = np.zeros((num_of_portfolios, number_of_sectors))\n",
    "\n",
    "# Prep an array to store the returns as they are generated, 5000 possible return values.\n",
    "ret_arr = np.zeros(num_of_portfolios)\n",
    "\n",
    "# Prep an array to store the volatilities as they are generated, 5000 possible volatility values.\n",
    "vol_arr = np.zeros(num_of_portfolios)\n",
    "\n",
    "# Prep an array to store the sharpe ratios as they are generated, 5000 possible Sharpe Ratios.\n",
    "sharpe_arr = np.zeros(num_of_portfolios)\n",
    "\n",
    "# Start the simulations.\n",
    "for ind in range(num_of_portfolios):\n",
    "\n",
    "    # First, calculate the weights.\n",
    "    weights = np.array(np.random.random(number_of_sectors))\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # Add the weights, to the `weights_arrays`.\n",
    "    all_weights[ind, :] = weights\n",
    "\n",
    "    # Calculate the expected log returns, and add them to the `returns_array`.\n",
    "    ret_arr[ind] = np.sum(((log_return.mean()-risk_free_rate) * weights) * 252)\n",
    "\n",
    "    # Calculate the volatility, and add them to the `volatility_array`.\n",
    "    vol_arr[ind] = np.sqrt(\n",
    "        np.dot(weights.T, np.dot(log_return.cov() * 252, weights)))        \n",
    "\n",
    "\n",
    "    # Calculate the Sharpe Ratio and Add it to the `sharpe_ratio_array`.\n",
    "    sharpe_arr[ind] = ret_arr[ind]/vol_arr[ind]\n",
    "\n",
    "# Let's create our \"Master Data Frame\", with the weights, the returns, the volatility, and the Sharpe Ratio\n",
    "simulations_data = [ret_arr, vol_arr, sharpe_arr, all_weights]\n",
    "\n",
    "# Create a DataFrame from it, then Transpose it so it looks like our original one.\n",
    "simulations_df = pd.DataFrame(data=simulations_data).T\n",
    "\n",
    "# Give the columns the Proper Names.\n",
    "simulations_df.columns = [\n",
    "    'Returns',\n",
    "    'Volatility',\n",
    "    'Sharpe Ratio',\n",
    "    'Portfolio Weights'\n",
    "]\n",
    "\n",
    "# Make sure the data types are correct, we don't want our floats to be strings.\n",
    "simulations_df = simulations_df.infer_objects()\n",
    "\n",
    "# Print out the results.\n",
    "print('')\n",
    "print('='*80)\n",
    "print('SIMULATIONS RESULT:')\n",
    "print('-'*80)\n",
    "\n",
    "# Print PORTFOLIO WEIGHTS\n",
    "print('='*80)\n",
    "print('PORTFOLIO WEIGHTS:')\n",
    "print('-'*80)\n",
    "print(simulations_df['Portfolio Weights'].head())\n",
    "print('-'*80)\n",
    "\n",
    "# Print PORTFOLIO METRICS\n",
    "print('='*80)\n",
    "print('PORTFOLIO METRICS:')\n",
    "print('-'*80)\n",
    "print(simulations_df[['Returns', 'Volatility', 'Sharpe Ratio']].head())\n",
    "print('-'*80)\n",
    "\n",
    "# Return the Max Sharpe Ratio from the run.\n",
    "max_sharpe_ratio = simulations_df.loc[simulations_df['Sharpe Ratio'].idxmax()]\n",
    "\n",
    "# Return the Min Volatility from the run.\n",
    "min_volatility = simulations_df.loc[simulations_df['Volatility'].idxmin()]\n",
    "\n",
    "print('')\n",
    "print('='*80)\n",
    "print('MAX SHARPE RATIO:')\n",
    "print('-'*80)\n",
    "print(max_sharpe_ratio)\n",
    "print('-'*80)\n",
    "\n",
    "print('')\n",
    "print('='*80)\n",
    "print('MIN VOLATILITY:')\n",
    "print('-'*80)\n",
    "print(min_volatility)\n",
    "print('-'*80)\n",
    "#### Machine learning time\n",
    "#We can further improve these portfolios by generating future data via machine learning.\n",
    "simulations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Unique values in 'Portfolio Weights' column:\")\n",
    "print(simulations_df[\"Portfolio Weights\"].unique())\n",
    "\n",
    "simulations_df[\"Portfolio Weights\"] = simulations_df[\"Portfolio Weights\"].apply(lambda x: np.concatenate(x) if isinstance(x, list) else x)\n",
    "X = simulations_df[[\"Volatility\", \"Sharpe Ratio\"]]\n",
    "y = np.concatenate(simulations_df[\"Portfolio Weights\"].to_numpy())\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
